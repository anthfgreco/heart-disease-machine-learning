{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from utils import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParameterTuning(x, y, clf, params, fold=5, repeat=100):\n",
    "    \n",
    "    selector = SelectPercentile()\n",
    "    pipeline = Pipeline([('selector', selector), ('clf', clf)])\n",
    "    params['selector__percentile'] = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    params['selector__score_func'] = [f_classif, chi2]\n",
    "    #rskf = RepeatedStratifiedKFold(n_splits=fold, n_repeats=repeat, random_state=random.randint(1, 1000000))\n",
    "    skf = StratifiedKFold(n_splits=fold, shuffle=True, random_state=random.randint(1, 100000))\n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    results = GridSearchCV(pipeline, params, scoring=scoring, refit='accuracy', cv=skf)\n",
    "    results.fit(x, y)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression(X, y):\n",
    "\n",
    "    clf = LogisticRegression(solver='saga', max_iter=10000)\n",
    "    params = {'clf__penalty': ['l2', 'l1'], 'clf__C': [0.01, 0.1, 1, 10]}\n",
    "    x = MinMaxScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(X, y):\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    params = {}\n",
    "    x = MinMaxScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X, y):\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    params = {'clf__n_neighbors': [5, 10, 13, 20], 'clf__weights': ['uniform', 'distance'], 'clf__p': [1, 2]}\n",
    "    x = MinMaxScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X, y):\n",
    "    \n",
    "    clf = SVC()\n",
    "    params = {'clf__C': [0.1, 1, 5, 10], 'clf__kernel': ['linear', 'poly', 'rbf'],\n",
    "              'clf__degree': [3, 4, 5, 6], 'clf__gamma': ['scale', 'auto']}\n",
    "    x = StandardScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTree(X, y):\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    params = {'clf__criterion': ['gini', 'entropy'], 'clf__max_depth': [None, 9, 8, 7, 6, 5], \n",
    "              'clf__min_samples_leaf': [1, 5, 10, 20], 'clf__max_features': [None, 'auto', 9, 8, 7, 6, 5],\n",
    "              'clf__ccp_alpha': [0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.5]}\n",
    "    x = StandardScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def RForest(X, y):\n",
    "    \n",
    "#     clf = RandomForestClassifier()\n",
    "#     params = {'clf__n_estimators': [10, 25, 50, 100], 'clf__criterion': ['gini', 'entropy'], \n",
    "#               'clf__max_depth': [None, 9, 8, 7, 6, 5], 'clf__min_samples_leaf': [1, 5, 10, 20], \n",
    "#               'clf__max_features': ['auto', 9, 8, 7, 6, 5]}\n",
    "#     x = StandardScaler().fit_transform(X)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "#     res = ParameterTuning(x_train, y_train, clf, params)\n",
    "#     y_pred_test = res.predict(x_test)\n",
    "#     y_pred_train = res.predict(x_train)\n",
    "#     acc_test = accuracy_score(y_test, y_pred_test)\n",
    "#     acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "#     return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ANN(X, y):\n",
    "    \n",
    "#     clf = MLPClassifier(max_iter=10000)\n",
    "#     params = {'clf__hidden_layer_sizes': [(100,), (10, 10), (10, 5, 2)], \n",
    "#               'clf__activation': ['identity', 'logistic', 'tanh', 'relu'], 'clf__solver': ['lbfgs', 'sgd', 'adam'], \n",
    "#               'clf__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1], \n",
    "#               'clf__learning_rate': ['constant', 'invscaling', 'adaptive'], 'clf__early_stopping': [True, False]}\n",
    "#     x = MinMaxScaler().fit_transform(X)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "#     res = ParameterTuning(x_train, y_train, clf, params)\n",
    "#     y_pred_test = res.predict(x_test)\n",
    "#     y_pred_train = res.predict(x_train)\n",
    "#     acc_test = accuracy_score(y_test, y_pred_test)\n",
    "#     acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "#     return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RForest(X, y):\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    params = {'clf__n_estimators': [10, 25, 50, 100], 'clf__criterion': ['gini', 'entropy'], \n",
    "              'clf__max_depth': [None, 9, 8, 7, 6, 5]}\n",
    "    x = StandardScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(X, y):\n",
    "    \n",
    "    clf = MLPClassifier(max_iter=10000)\n",
    "    params = {'clf__hidden_layer_sizes': [(100,), (10, 10), (10, 5, 2)], \n",
    "              'clf__activation': ['relu'], 'clf__solver': ['adam'], \n",
    "              'clf__alpha': [0.0001, 0.001, 0.01], 'clf__early_stopping': [True, False]}\n",
    "    x = MinMaxScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random.randint(1, 100000))\n",
    "    res = ParameterTuning(x_train, y_train, clf, params)\n",
    "    y_pred_test = res.predict(x_test)\n",
    "    y_pred_train = res.predict(x_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    return res, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Done!\n",
      "Naive Bayes Done!\n",
      "KNN Done!\n",
      "SVM Done!\n",
      "Decision Tree Done!\n"
     ]
    }
   ],
   "source": [
    "x, y = load_dataset(\"heart_processed_noC.csv\")\n",
    "\n",
    "lr_res, lr_acc_tr, lr_acc_te = Logistic_Regression(x, y)\n",
    "print('Logistic Regression Done!')\n",
    "\n",
    "nb_res, nb_acc_tr, nb_acc_te = NaiveBayes(x, y)\n",
    "print('Naive Bayes Done!')\n",
    "\n",
    "knn_res, knn_acc_tr, knn_acc_te = KNN(x, y)\n",
    "print('KNN Done!')\n",
    "\n",
    "svm_res, svm_acc_tr, svm_acc_te = SVM(x, y)\n",
    "print('SVM Done!')\n",
    "\n",
    "dtree_res, dtree_acc_tr, dtree_acc_te = DTree(x, y)\n",
    "print('Decision Tree Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Train: 84.47%    Test: 87.5%\n",
      " \n",
      "Naive Bayes:\n",
      "Train: 85.01%    Test: 84.24%\n",
      " \n",
      "k-Nearest Neighbours:\n",
      "Train: 89.1%    Test: 85.33%\n",
      " \n",
      "Support Vector Machines:\n",
      "Train: 89.37%    Test: 88.04%\n",
      " \n",
      "Decision Tree:\n",
      "Train: 85.69%    Test: 84.78%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression:')\n",
    "print('Train: {}%    Test: {}%'.format(round(lr_acc_tr*100, 2), round(lr_acc_te*100, 2)))\n",
    "print(' ')\n",
    "print('Naive Bayes:')\n",
    "print('Train: {}%    Test: {}%'.format(round(nb_acc_tr*100, 2), round(nb_acc_te*100, 2)))\n",
    "print(' ')\n",
    "print('k-Nearest Neighbours:')\n",
    "print('Train: {}%    Test: {}%'.format(round(knn_acc_tr*100, 2), round(knn_acc_te*100, 2)))\n",
    "print(' ')\n",
    "print('Support Vector Machines:')\n",
    "print('Train: {}%    Test: {}%'.format(round(svm_acc_tr*100, 2), round(svm_acc_te*100, 2)))\n",
    "print(' ')\n",
    "print('Decision Tree:')\n",
    "print('Train: {}%    Test: {}%'.format(round(dtree_acc_tr*100, 2), round(dtree_acc_te*100, 2)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Best Params: {'clf__C': 10, 'clf__penalty': 'l2', 'selector__percentile': 60, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 84.46\n",
      " \n",
      "Naive Bayes:\n",
      "Best Params: {'selector__percentile': 70, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 84.74\n",
      " \n",
      "k-Nearest Neighbours:\n",
      "Best Params: {'clf__n_neighbors': 5, 'clf__p': 1, 'clf__weights': 'uniform', 'selector__percentile': 100, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 85.97\n",
      " \n",
      "Support Vector Machines:\n",
      "Best Params: {'clf__C': 1, 'clf__degree': 3, 'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'selector__percentile': 90, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 86.37\n",
      " \n",
      "Decision Tree:\n",
      "Best Params: {'clf__ccp_alpha': 0.01, 'clf__criterion': 'gini', 'clf__max_depth': 5, 'clf__max_features': 5, 'clf__min_samples_leaf': 1, 'selector__percentile': 90, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 84.88\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression:')\n",
    "print('Best Params: {}'.format(lr_res.best_params_))\n",
    "print('Best Score: {}'.format(round(lr_res.best_score_*100, 2)))\n",
    "print(' ')\n",
    "print('Naive Bayes:')\n",
    "print('Best Params: {}'.format(nb_res.best_params_))\n",
    "print('Best Score: {}'.format(round(nb_res.best_score_*100, 2)))\n",
    "print(' ')\n",
    "print('k-Nearest Neighbours:')\n",
    "print('Best Params: {}'.format(knn_res.best_params_))\n",
    "print('Best Score: {}'.format(round(knn_res.best_score_*100, 2)))\n",
    "print(' ')\n",
    "print('Support Vector Machines:')\n",
    "print('Best Params: {}'.format(svm_res.best_params_))\n",
    "print('Best Score: {}'.format(round(svm_res.best_score_*100, 2)))\n",
    "print(' ')\n",
    "print('Decision Tree:')\n",
    "print('Best Params: {}'.format(dtree_res.best_params_))\n",
    "print('Best Score: {}'.format(round(dtree_res.best_score_*100, 2)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Done!\n"
     ]
    }
   ],
   "source": [
    "x, y = load_dataset(\"heart_processed_noC.csv\")\n",
    "\n",
    "ann_res, ann_acc_tr, ann_acc_te = ANN(x, y)\n",
    "print('ANN Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifical Neural Network:\n",
      "Train: 87.19%    Test: 82.07%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Artifical Neural Network:')\n",
    "print('Train: {}%    Test: {}%'.format(round(ann_acc_tr*100, 2), round(ann_acc_te*100, 2)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifical Neural Network:\n",
      "Best Params: {'clf__activation': 'relu', 'clf__alpha': 0.001, 'clf__early_stopping': False, 'clf__hidden_layer_sizes': (10, 5, 2), 'clf__solver': 'adam', 'selector__percentile': 80, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 87.06\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Artifical Neural Network:')\n",
    "print('Best Params: {}'.format(ann_res.best_params_))\n",
    "print('Best Score: {}'.format(round(ann_res.best_score_*100, 2)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Done!\n"
     ]
    }
   ],
   "source": [
    "rf_res, rf_acc_tr, rf_acc_te = RForest(x, y)\n",
    "print('Random Forest Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Train: 98.09%    Test: 84.78%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Random Forest:')\n",
    "print('Train: {}%    Test: {}%'.format(round(rf_acc_tr*100, 2), round(rf_acc_te*100, 2)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Best Params: {'clf__criterion': 'gini', 'clf__max_depth': 9, 'clf__n_estimators': 50, 'selector__percentile': 100, 'selector__score_func': <function f_classif at 0x7fe19a8d38c0>}\n",
      "Best Score: 87.46\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Random Forest:')\n",
    "print('Best Params: {}'.format(rf_res.best_params_))\n",
    "print('Best Score: {}'.format(round(rf_res.best_score_*100, 2)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
